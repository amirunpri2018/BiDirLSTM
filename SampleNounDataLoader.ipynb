{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterHighFreqNoun(pos_dir:str, split_name:str, idx_to_keep:list):\n",
    "    print('\\tchecking highest freq noun...')\n",
    "    total_num_sent_noun1 = 0\n",
    "    noun_to_sent_idx_mapping = dict()\n",
    "    \n",
    "    with os.scandir(pos_dir) as it:\n",
    "        for entry in it:\n",
    "            if entry.name.startswith(split_name): # load all pos info with given split name\n",
    "                with open(entry.path, 'r', encoding='utf-8') as textfile:\n",
    "                    pos_info = json.load(textfile)\n",
    "                    for (sent_idx, sent_pos) in pos_info.items():\n",
    "                        sent_idx = int(sent_idx)\n",
    "                        if sent_idx in idx_to_keep: # ignore sentences based on idx filter\n",
    "                            for token_pos in sent_pos:\n",
    "                                if token_pos['pos'] == 'NN' or token_pos['pos'] == 'NNS':\n",
    "                                    # only care for NN and NNS for now, ignore pronouns NNP\n",
    "                                    # the token index given by stanford pos starts with 1 in a sent\n",
    "                                    word = token_pos['word']\n",
    "                                    token_idx = token_pos['index'] - 1\n",
    "                                    if not word in noun_to_sent_idx_mapping:\n",
    "                                        noun_to_sent_idx_mapping[word] = list()\n",
    "                                    noun_to_sent_idx_mapping[word].append([sent_idx, token_idx])\n",
    "    \n",
    "    # filter out the noun with the most frequent appearance from sorting\n",
    "    sorted_noun_to_sent_idx_mapping = sorted(noun_to_sent_idx_mapping.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    noun_to_sent_idx_mapping = dict()\n",
    "    sent_idx_filter = list()\n",
    "    sent_idx_to_noun_idx_mapping = dict()\n",
    "    \n",
    "    for i in range(10):\n",
    "        highfreq_noun = sorted_noun_to_sent_idx_mapping[i][0]\n",
    "        highfreq_position = sorted_noun_to_sent_idx_mapping[i][1]\n",
    "        for (sent_idx, noun_idx) in highfreq_position:\n",
    "            if sent_idx not in sent_idx_filter:\n",
    "                sent_idx_filter.append(sent_idx)\n",
    "                sent_idx_to_noun_idx_mapping[sent_idx] = list()\n",
    "            sent_idx_to_noun_idx_mapping[sent_idx].append(noun_idx)\n",
    "        noun_to_sent_idx_mapping[highfreq_noun] = highfreq_position\n",
    "    \n",
    "    with open('top10noun_with_idx.json', 'w', encoding='utf-8') as textfile:\n",
    "        json.dump(noun_to_sent_idx_mapping, textfile)\n",
    "    \n",
    "    return noun_to_sent_idx_mapping.keys(), sent_idx_filter, sent_idx_to_noun_idx_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDataloaderNoun(filteridx_filename:str, sent_filename:str, pos_dir:str, split_name:str, glove_model, embedding_dim:int, max_len:int, batch_size:int, num_workers:int):\n",
    "    print('making data loader for {} split'.format(split_name))\n",
    "    idx_to_keep = LoadFilterIdxFile(filteridx_filename)\n",
    "    nouns, sent_idx_filter, sent_idx_to_noun_idx_mapping = FilterHighFreqNoun(pos_dir, split_name, idx_to_keep)\n",
    "    idx_to_sentence_mapping = LoadSentenceFile(sent_filename, sent_idx_filter)\n",
    "    sentence_embeddings, label_embeddings, label_positions = SentToEmbedding(idx_to_sentence_mapping, sent_idx_to_noun_idx_mappping, glove_model, embedding_dim, max_len, split_name)\n",
    "    data = WikiDataset(sentence_embeddings, label_embeddings, label_positions)\n",
    "    \n",
    "    print('\\tfinalizing making data loader, num sentences in loader: {}'.format(len(label_positions)))\n",
    "    \n",
    "    if split_name == 'test':\n",
    "        if_shuffle = False\n",
    "    else:\n",
    "        if_shuffle = True\n",
    "        \n",
    "    # NOTE: for gpu usage, do not use num_workers, turn pin_memory to True\n",
    "    loader = DataLoader(dataset=data, batch_size=batch_size, shuffle=if_shuffle, num_workers=num_workers, pin_memory=False) \n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample noun from train\n",
    "split_name = 'train'\n",
    "filter_path = './dataset_filtered/{}_idx_filtered.txt'.format(split_name)\n",
    "file_path = './dataset_filtered/{}.json'.format(split_name)\n",
    "pos_dir = './dataset_pos'\n",
    "embedding_dim = 300\n",
    "batch_size = 128\n",
    "num_workers = 1\n",
    "max_len = int(22.514047250226433 + 14.624483763629705)\n",
    "nounloader = MakeDataloaderNoun(filter_path, file_path, pos_dir, split_name, glove_model, embedding_dim, max_len, batch_size, num_workers)\n",
    "print(len(nounloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
